{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, Audio,concatenate_datasets\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import torchaudio\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca22c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366607c419314a6bb7b70536382c77d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa785dde",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2837f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generated by Gemini to get the paths\n",
    "\n",
    "def get_audio_file_paths(base_path_str: str) -> dict:\n",
    "    \n",
    "    base_path = Path(base_path_str)\n",
    "    processed_dir = base_path / \"processed data\"\n",
    "    \n",
    "    audio_paths = {}\n",
    "    audio_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.ogg', '.opus'}\n",
    "\n",
    "    if not processed_dir.is_dir():\n",
    "        return audio_paths\n",
    "\n",
    "    for lang_dir in processed_dir.iterdir():\n",
    "        if not lang_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        lang_name = lang_dir.name\n",
    "        audio_paths[lang_name] = {}\n",
    "        \n",
    "        for sub_dir in lang_dir.iterdir():\n",
    "            if not sub_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            sub_name = sub_dir.name\n",
    "            \n",
    "            files = [\n",
    "                str(f.resolve()) for f in sub_dir.glob('*') \n",
    "                if f.is_file() and f.suffix.lower() in audio_extensions\n",
    "            ]\n",
    "            audio_paths[lang_name][sub_name] = files\n",
    "            \n",
    "    return audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/workspace/whisper-math/data\"\n",
    "todos_os_arquivos = get_audio_file_paths(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979454ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntext_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\\ntext_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ [CMD] ØªØ£ÙƒÙŠØ¯ [CMD] Ø£Ø¹ÙØ¯ [CMD] calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚ [CHK]\"\\n\\ntext_b_english = \\'one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start [CMD] stop [CMD] undo [CMD]\\'\\ntext_b_arabic = \\'Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© [CMD] ØªÙ… [CMD] ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© [CMD] calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø± [CHK]\\'\\n\\ntext_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm [CMD] repeat last [CMD] slower please [CMD]\"\\ntext_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ [CMD] Ø±Ø¬ÙˆØ¹ [CMD] Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ [CMD] calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø± [CHK]\"\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "text_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\n",
    "text_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ [CMD] ØªØ£ÙƒÙŠØ¯ [CMD] Ø£Ø¹ÙØ¯ [CMD] calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚ [CHK]\"\n",
    "\n",
    "text_b_english = 'one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start [CMD] stop [CMD] undo [CMD]'\n",
    "text_b_arabic = 'Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© [CMD] ØªÙ… [CMD] ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© [CMD] calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø± [CHK]'\n",
    "\n",
    "text_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm [CMD] repeat last [CMD] slower please [CMD]\"\n",
    "text_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ [CMD] Ø±Ø¬ÙˆØ¹ [CMD] Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ [CMD] calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø± [CHK]\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6577ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\n",
    "text_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ ØªØ£ÙƒÙŠØ¯ Ø£Ø¹ÙØ¯ calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚\"\n",
    "\n",
    "text_b_english = \"one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start stop undo\"\n",
    "text_b_arabic = \"Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© ØªÙ… ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø±\"\n",
    "\n",
    "text_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm repeat last slower please\"\n",
    "text_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ Ø±Ø¬ÙˆØ¹ Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø±\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6442929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_audio_file_paths(data_path)\n",
    "arabic_files = all_files['arabic']\n",
    "english_files = all_files['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32068915",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_arabic = {\n",
    "'A': text_a_arabic,\n",
    "'B': text_b_arabic,\n",
    "'C': text_c_arabic\n",
    "}\n",
    "\n",
    "transcriptions_english = {\n",
    "'A': text_a_english,\n",
    "'B': text_b_english,\n",
    "'C': text_c_english\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8829b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_g(transcriptions, files, language : str):\n",
    "\n",
    "    rows = []\n",
    "    for label in files:\n",
    "        text = transcriptions[label]  # uma Ãºnica string\n",
    "        for file_path in files[label]:\n",
    "            rows.append({'Label': label, 'audio': file_path, 'transcription': text, 'Language': language})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    df.drop('Label', axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70e77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arabic = dataset_g(transcriptions_arabic, arabic_files, language='arabic')\n",
    "df_english = dataset_g(transcriptions_english, english_files, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270475f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_dataset(df_arabic, df_english, augment_factor=1):\n",
    "  \n",
    "\n",
    "    # Define as augmentations\n",
    "    augment = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.0001, max_amplitude=0.005, p=0.3),\n",
    "        TimeStretch(min_rate=0.95, max_rate=1.05, p=0.3),\n",
    "        PitchShift(min_semitones=-1, max_semitones=1, p=0.3),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Cria o dataset base (lazy load)\n",
    "    dataset_arabic = Dataset.from_pandas(df_arabic).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    dataset_english = Dataset.from_pandas(df_english).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "    # FunÃ§Ã£o que aplica augment em cada item\n",
    "    def augment_audio(batch):\n",
    "        audio_array = batch[\"audio\"][\"array\"]\n",
    "        if isinstance(audio_array, np.ndarray):\n",
    "            augmented = augment(samples=audio_array, sample_rate=16000)\n",
    "            batch[\"audio\"] = {\"array\": augmented, \"sampling_rate\": 16000}\n",
    "        return batch\n",
    "\n",
    "    # Lista com todas as versÃµes\n",
    "    datasets_all = [dataset_arabic, dataset_english]\n",
    "\n",
    "    \n",
    "    # Cria as versÃµes aumentadas\n",
    "    for _ in range(augment_factor):\n",
    "        ds_aug = dataset_arabic.map(augment_audio)\n",
    "        datasets_all.append(ds_aug)\n",
    "\n",
    "    # Concatena tudo corretamente\n",
    "    full_dataset = concatenate_datasets(datasets_all)\n",
    "\n",
    "    return full_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76666b",
   "metadata": {},
   "source": [
    "## Generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25da24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151d5bdaeefe469789b71f0a8213e77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5eb1c3f8174293a770ace54af10d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'Language'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = generate_audio_dataset(df_arabic, df_english, augment_factor=2)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "831752c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'Language'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1610a99",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00bc0b",
   "metadata": {},
   "source": [
    "### Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c660c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0a60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-medium\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, task=\"transcribe\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model.freeze_encoder() # because our dataset is small\n",
    "\n",
    "# Desativar idioma fixo (importantÃ­ssimo)\n",
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=16000\n",
    "        ).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeb8a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5863df5a2894ddb807f62db0fa5b302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef789d57a714173ba968be00ea9ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = df_final.map(preprocess_function, remove_columns=df_final[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943f4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e39a58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02baf0",
   "metadata": {},
   "source": [
    "### Defining evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9183f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81568b2b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_9946/696726641.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 22:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.275652</td>\n",
       "      <td>35.595777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.044318</td>\n",
       "      <td>34.351433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.041492</td>\n",
       "      <td>37.933635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>35.218703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>31.749623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.033641</td>\n",
       "      <td>33.634992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>32.918552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>33.597285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.031108</td>\n",
       "      <td>31.184012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>31.184012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/venv/main/lib/python3.12/site-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.10243829224695218, metrics={'train_runtime': 1363.3385, 'train_samples_per_second': 0.88, 'train_steps_per_second': 0.44, 'total_flos': 1.224725889024e+18, 'train_loss': 0.10243829224695218, 'epoch': 10.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-medium-finetuned\",\n",
    "\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-5, \n",
    "    num_train_epochs=10, # best number of epochs!\n",
    "    warmup_steps=30,\n",
    "\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "\n",
    "    predict_with_generate=True,           \n",
    "    generation_max_length=150,         \n",
    "\n",
    "    dataloader_num_workers=0,             \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"]\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f289df",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f4e3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_meu = torchaudio.load(\"/workspace/whisper-math/english by me.m4a\")\n",
    "audio_arabic = torchaudio.load(\"/workspace/whisper-math/data/processed data/arabic/A/arabic 11.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29529fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
    "audio_meu = resampler(audio_meu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "521395f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_arabic = resampler(audio_arabic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ee271a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ar\", task=\"transcribe\")\n",
    "\n",
    "# load streaming dataset and read first audio sample\n",
    "input_features = processor(audio_arabic[0], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2c5d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449f77ceed434aff8a971b6bcc6ba50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97f6372af5a4cb488a7a0f09daa6e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17c8bc7dcd24cfb8e60dea4330a4e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-medium-finetuned/model.safetensors:   0%|          |  947kB / 3.06GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0be981fa49f4d8ca5686b6931d1e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ut.tfevents.1762659655.fa67f1519dc7:  15%|#4        | 3.35kB / 22.8kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0048aa6c8b83416aa45d28bee2cb184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ut.tfevents.1762661130.fa67f1519dc7:  15%|#4        |  52.0B /   358B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a51e7fcada48dfb47779851d9a456c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-medium-finetuned/training_args.bin:  15%|#4        |   865B / 5.91kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/manushya-ai/whisper-medium-finetuned/commit/d252ef01a33492355b333108816e658fe05897c7', commit_message='End of training', commit_description='', oid='d252ef01a33492355b333108816e658fe05897c7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/manushya-ai/whisper-medium-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='manushya-ai/whisper-medium-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
