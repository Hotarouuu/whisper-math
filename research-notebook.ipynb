{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81af6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, Audio,concatenate_datasets\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import torchaudio\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca22c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab9c1772ea4d52b582b20761da5d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa785dde",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2837f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generated by Gemini to get the paths\n",
    "\n",
    "def get_audio_file_paths(base_path_str: str) -> dict:\n",
    "    \n",
    "    base_path = Path(base_path_str)\n",
    "    processed_dir = base_path / \"processed data\"\n",
    "    \n",
    "    audio_paths = {}\n",
    "    audio_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.ogg', '.opus'}\n",
    "\n",
    "    if not processed_dir.is_dir():\n",
    "        return audio_paths\n",
    "\n",
    "    for lang_dir in processed_dir.iterdir():\n",
    "        if not lang_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        lang_name = lang_dir.name\n",
    "        audio_paths[lang_name] = {}\n",
    "        \n",
    "        for sub_dir in lang_dir.iterdir():\n",
    "            if not sub_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            sub_name = sub_dir.name\n",
    "            \n",
    "            files = [\n",
    "                str(f.resolve()) for f in sub_dir.glob('*') \n",
    "                if f.is_file() and f.suffix.lower() in audio_extensions\n",
    "            ]\n",
    "            audio_paths[lang_name][sub_name] = files\n",
    "            \n",
    "    return audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"/workspace/whisper-math/data\"\n",
    "todos_os_arquivos = get_audio_file_paths(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979454ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntext_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\\ntext_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ [CMD] ØªØ£ÙƒÙŠØ¯ [CMD] Ø£Ø¹ÙØ¯ [CMD] calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚ [CHK]\"\\n\\ntext_b_english = \\'one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start [CMD] stop [CMD] undo [CMD]\\'\\ntext_b_arabic = \\'Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© [CMD] ØªÙ… [CMD] ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© [CMD] calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø± [CHK]\\'\\n\\ntext_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm [CMD] repeat last [CMD] slower please [CMD]\"\\ntext_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ [CMD] Ø±Ø¬ÙˆØ¹ [CMD] Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ [CMD] calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø± [CHK]\"\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "text_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\n",
    "text_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ [CMD] ØªØ£ÙƒÙŠØ¯ [CMD] Ø£Ø¹ÙØ¯ [CMD] calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚ [CHK]\"\n",
    "\n",
    "text_b_english = 'one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start [CMD] stop [CMD] undo [CMD]'\n",
    "text_b_arabic = 'Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© [CMD] ØªÙ… [CMD] ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© [CMD] calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø± [CHK]'\n",
    "\n",
    "text_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm [CMD] repeat last [CMD] slower please [CMD]\"\n",
    "text_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ [CMD] Ø±Ø¬ÙˆØ¹ [CMD] Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ [CMD] calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø± [CHK]\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6577ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_a_english = \"zero five twelve ninety-nine one hundred and five 2 plus 7 18 minus 4 6 times 3 20 divided by 5 ten plus thirty minus eight negative fifteen plus nine three to the power of two square root of sixteen clear equals repeat\"\n",
    "text_a_arabic = \"Ø§Ø­Ø³Ø¨ Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ Ø§Ø«Ù†ÙŠÙ† Ø¹Ø´Ø±Ø© Ù†Ø§Ù‚Øµ Ø«Ù„Ø§Ø«Ø© Ø³ØªØ© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ø¹Ø´Ø±ÙˆÙ† Ù‚Ø³Ù…Ø© Ø®Ù…Ø³Ø© Ø³Ø§Ù„Ø¨ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ Ø®Ù…Ø³Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„Ø£Ø±Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ ØªØ£ÙƒÙŠØ¯ Ø£Ø¹ÙØ¯ calculate 37 plus Ø®Ù…Ø³Ø© Ø§Ø·Ø±Ø­ twelve Ù…Ù† Ø¹Ø´Ø±Ø© Ø§Ø¶Ø±Ø¨ Ø«Ù„Ø§Ø«Ø© ÙÙŠ twenty eighty divided by Ø«Ù…Ø§Ù†ÙŠØ© Ø§Ø¬Ù…Ø¹ Ù¡Ù¢ Ùˆ Ù¡Ù£ Ø³Ø¨Ø¹Ø© Ø²Ø§Ø¦Ø¯ Ù¡Ù© 45 minus ØªØ³Ø¹Ø© 3.5 plus Ø§Ø«Ù†ÙŠÙ† ÙˆÙ†ØµÙ ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø¶Ø±Ø¨ Ø£Ø±Ø¨Ø¹Ø© Ù…ÙŠØ© ÙˆØ§Ø«Ù†Ø§ Ø¹Ø´Ø± Ù†Ø§Ù‚Øµ Ø³ØªØ© 1000 minus 250 999 plus 1 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø²Ø±Ù‚\"\n",
    "\n",
    "text_b_english = \"one eight seventeen sixty-four one hundred and twenty 4 plus 9 22 minus 7 9 times 5 81 divided by 9 thirty plus fifty negative six minus ten plus three two to the power of five cube root of twenty-seven start stop undo\"\n",
    "text_b_arabic = \"Ø§Ø¬Ù…Ø¹ Ø³Ø¨Ø¹Ø© Ùˆ ØªÙ„Ø§ØªÙŠÙ† Ù…Ø¹ 12 Ø®Ù…Ø³Ø© ÙˆØ£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±ÙŠÙ† ØªØ³Ø¹Ø© Ø¶Ø±Ø¨ Ø³ØªØ© Ø£Ø±Ø¨Ø¹Ø© ÙˆØ³ØªÙˆÙ† Ù‚Ø³Ù…Ø© Ø«Ù…Ø§Ù†ÙŠØ© Ø³Ø§Ù„Ø¨ Ø«Ù„Ø§Ø«Ø© Ø²Ø§Ø¦Ø¯ Ø®Ù…Ø³Ø© Ø§Ø«Ù†Ø§Ù† Ø£Ø³ Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªÙƒØ¹ÙŠØ¨ÙŠ Ù„Ø³Ø¨Ø¹Ø© ÙˆØ¹Ø´Ø±ÙŠÙ† Ø§Ù…Ø³Ø­ Ø§Ù„Ø´Ø§Ø´Ø© ØªÙ… ÙƒØ±Ø± Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© calculate twelve times Ø®Ù…Ø³Ø© Ø§Ù‚Ø³Ù… 36 Ø¹Ù„Ù‰ Ø³ØªØ© Ø§Ø·Ø±Ø­ Ø®Ù…Ø³Ø© Ù…Ù† twenty fifty plus Ø³Ø¨Ø¹Ø© Ø§Ø¬Ù…Ø¹ Ù¡Ù Ù  Ùˆ Ù¢Ù¥ Ù…Ø¦ØªØ§Ù† Ù†Ø§Ù‚Øµ Ù©Ù© 14 minus Ø£Ø±Ø¨Ø¹Ø© Ø§Ø«Ù†ÙŠÙ† ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ø²Ø§Ø¦Ø¯ 0.5 7.25 divided by Ø®Ù…Ø³Ø© Ø£Ø±Ø¨Ø¹ Ù…ÙŠØ© ÙˆØ®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ Ø¹Ø´Ø±Ø© 500 plus 500 1234 minus 234 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø®Ø¶Ø±\"\n",
    "\n",
    "text_c_english = \"two nine eleven seventy-three two hundred and three 8 plus 6 40 minus 12 7 times 7 90 divided by 10 twenty plus fifteen negative nine minus twenty plus eight five to the power of three square root of one hundred confirm repeat last slower please\"\n",
    "text_c_arabic = \"Ø§Ø­Ø³Ø¨ 23 Ø²Ø§Ø¦Ø¯ 15 Ø³Ø¨Ø¹Ø© Ù†Ø§Ù‚Øµ Ø§Ø«Ù†ÙŠÙ† Ø«Ù„Ø§Ø«Ø© Ø¶Ø±Ø¨ ØªØ³Ø¹Ø© Ø³ØªØ© ÙˆØ«Ù„Ø§Ø«ÙˆÙ† Ù‚Ø³Ù…Ø© Ø£Ø±Ø¨Ø¹Ø© Ø³Ø§Ù„Ø¨ Ø§Ø«Ù†Ø§ Ø¹Ø´Ø± Ø²Ø§Ø¦Ø¯ Ø¹Ø´Ø±Ø© Ø¹Ø´Ø±Ø© Ø£Ø³ Ø§Ø«Ù†ÙŠÙ† Ø§Ù„Ø¬Ø°Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ Ù„ØªØ³Ø¹Ø© Ø§ÙØªØ­ Ø±Ø¬ÙˆØ¹ Ø£Ø¹Ø¯ Ø§Ù„Ø­Ø³Ø§Ø¨ calculate twenty minus Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ five Ùˆ Ø®Ù…Ø³Ø© Ø§Ø¶Ø±Ø¨ 8 ÙÙŠ twenty-one thirty divided by Ø«Ù„Ø§Ø«Ø© Ø§Ø¬Ù…Ø¹ Ù§ Ùˆ Ù¡Ù¡ Ø£Ø±Ø¨Ø¹ÙˆÙ† Ù†Ø§Ù‚Øµ Ù¡Ù¨ 16 plus Ø³Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ ÙØ§ØµÙ„Ø© Ø®Ù…Ø³Ø© Ù†Ø§Ù‚Øµ 0.25 2.2 times Ø§Ø«Ù†ÙŠÙ† ØªØ³Ø¹Ù…ÙŠØ© ÙˆØªØ³Ø¹Ø© ÙˆØªØ³Ø¹ÙŠÙ† Ø²Ø§Ø¦Ø¯ ÙˆØ§Ø­Ø¯ 1500 minus 300 333 plus 667 Ù‚Ù„ Ø§Ù„Ù„ÙˆÙ†: Ø£Ø­Ù…Ø±\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6442929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_audio_file_paths(data_path)\n",
    "arabic_files = all_files['arabic']\n",
    "english_files = all_files['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32068915",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_arabic = {\n",
    "'A': text_a_arabic,\n",
    "'B': text_b_arabic,\n",
    "'C': text_c_arabic\n",
    "}\n",
    "\n",
    "transcriptions_english = {\n",
    "'A': text_a_english,\n",
    "'B': text_b_english,\n",
    "'C': text_c_english\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8829b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_g(transcriptions, files, language : str):\n",
    "\n",
    "    rows = []\n",
    "    for label in files:\n",
    "        text = transcriptions[label]  # uma Ãºnica string\n",
    "        for file_path in files[label]:\n",
    "            rows.append({'Label': label, 'audio': file_path, 'transcription': text, 'Language': language})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    df.drop('Label', axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70e77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arabic = dataset_g(transcriptions_arabic, arabic_files, language='arabic')\n",
    "df_english = dataset_g(transcriptions_english, english_files, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270475f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_dataset(df_arabic, df_english, augment_factor=1):\n",
    "  \n",
    "\n",
    "    # Define as augmentations\n",
    "    augment = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.0001, max_amplitude=0.005, p=0.3),\n",
    "        TimeStretch(min_rate=0.95, max_rate=1.05, p=0.3),\n",
    "        PitchShift(min_semitones=-1, max_semitones=1, p=0.3),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Cria o dataset base (lazy load)\n",
    "    dataset_arabic = Dataset.from_pandas(df_arabic).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    dataset_english = Dataset.from_pandas(df_english).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "    # FunÃ§Ã£o que aplica augment em cada item\n",
    "    def augment_audio(batch):\n",
    "        audio_array = batch[\"audio\"][\"array\"]\n",
    "        if isinstance(audio_array, np.ndarray):\n",
    "            augmented = augment(samples=audio_array, sample_rate=16000)\n",
    "            batch[\"audio\"] = {\"array\": augmented, \"sampling_rate\": 16000}\n",
    "        return batch\n",
    "\n",
    "    # Lista com todas as versÃµes\n",
    "    datasets_all = [dataset_arabic, dataset_english]\n",
    "\n",
    "    \n",
    "    # Cria as versÃµes aumentadas\n",
    "    for _ in range(augment_factor):\n",
    "        ds_aug = dataset_arabic.map(augment_audio)\n",
    "        datasets_all.append(ds_aug)\n",
    "\n",
    "    # Concatena tudo corretamente\n",
    "    full_dataset = concatenate_datasets(datasets_all)\n",
    "\n",
    "    return full_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76666b",
   "metadata": {},
   "source": [
    "## Generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25da24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807a14f103294605b76bebd15e6e3983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd14b3bf5204309958e267f604edb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'Language'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = generate_audio_dataset(df_arabic, df_english, augment_factor=2)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "831752c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'Language'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1610a99",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00bc0b",
   "metadata": {},
   "source": [
    "### Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c660c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0a60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aaf1d5b3fbf4293a4935f0c44771513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b945946b5104ae987002dbc50f33f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d69ed558474ceaa2cdca9107befb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9038180d15ac4946b7051ac5c1224058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecfecf2d1614cc4b7d980f0352e753f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb69e6bad1e42f6a9c82b506fe2a47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9240c3a67e349c5b95e353fded301ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deea3ae08ef4f4b95ba2a1fb43e9617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65cf4eca2434c1f8d82bbfc1c88ab65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f567164692ed496690d3ff4962e8b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108921fc2a8446cda315e8180df44b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"openai/whisper-medium\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name, task=\"transcribe\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "#model.freeze_encoder() # because our dataset is small\n",
    "\n",
    "# Desativar idioma fixo (importantÃ­ssimo)\n",
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91dbddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=16000\n",
    "        ).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeb8a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d28bd7446c4d2db107429b283212f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ee771056dd46c48daf23869c36c8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = df_final.map(preprocess_function, remove_columns=df_final[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943f4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e39a58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c02baf0",
   "metadata": {},
   "source": [
    "### Defining evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9183f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bcf5418b9941d3977cd30261cac870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81568b2b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bf6220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1070/3497150756.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/600 21:36 < 01:45, 0.43 it/s, Epoch 9.23/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.137001</td>\n",
       "      <td>48.826816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.052183</td>\n",
       "      <td>40.633147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>34.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>29.124767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>28.491620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.012788</td>\n",
       "      <td>31.769088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>28.864060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>28.044693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>28.715084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/venv/main/lib/python3.12/site-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m      1\u001b[39m training_args = Seq2SeqTrainingArguments(\n\u001b[32m      2\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./whisper-medium-finetuned\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     push_to_hub=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     31\u001b[39m     args=training_args,\n\u001b[32m     32\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     tokenizer=processor.feature_extractor,\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2547\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2542\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2543\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalculated loss must be on the original device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but device in use is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss_step.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         )\n\u001b[32m   2545\u001b[39m     tr_loss = tr_loss + tr_loss_step\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m \u001b[38;5;28mself\u001b[39m.current_flos += \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_sync_step:\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# Since we perform prefetching, we need to manually set sync_gradients to True\u001b[39;00m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.accelerator.gradient_state._set_sync_gradients(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:4528\u001b[39m, in \u001b[36mTrainer.floating_point_ops\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   4515\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4516\u001b[39m \u001b[33;03mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[32m   4517\u001b[39m \u001b[33;03moperations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4525\u001b[39m \u001b[33;03m    `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[32m   4526\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfloating_point_ops\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4529\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/modeling_utils.py:1212\u001b[39m, in \u001b[36mModuleUtilsMixin.floating_point_ops\u001b[39m\u001b[34m(self, input_dict, exclude_embeddings)\u001b[39m\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfloating_point_ops\u001b[39m(\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28mself\u001b[39m, input_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch.Tensor, Any]], exclude_embeddings: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1190\u001b[39m ) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1192\u001b[39m \u001b[33;03m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[39;00m\n\u001b[32m   1193\u001b[39m \u001b[33;03m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1209\u001b[39m \u001b[33;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[32m   1210\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m6\u001b[39m * \u001b[38;5;28mself\u001b[39m.estimate_tokens(input_dict) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/modeling_utils.py:1133\u001b[39m, in \u001b[36mModuleUtilsMixin.num_parameters\u001b[39m\u001b[34m(self, only_trainable, exclude_embeddings)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exclude_embeddings:\n\u001b[32m   1129\u001b[39m     embedding_param_names = [\n\u001b[32m   1130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.weight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_type, nn.Embedding)\n\u001b[32m   1131\u001b[39m     ]\n\u001b[32m   1132\u001b[39m     total_parameters = \u001b[43m[\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding_param_names\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     total_parameters = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.parameters())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:2711\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2684\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2685\u001b[39m \n\u001b[32m   2686\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2703\u001b[39m \n\u001b[32m   2704\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2705\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2706\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2707\u001b[39m     prefix=prefix,\n\u001b[32m   2708\u001b[39m     recurse=recurse,\n\u001b[32m   2709\u001b[39m     remove_duplicate=remove_duplicate,\n\u001b[32m   2710\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2711\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:2654\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2652\u001b[39m     memo.add(v)\n\u001b[32m   2653\u001b[39m name = module_prefix + (\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + k\n\u001b[32m-> \u001b[39m\u001b[32m2654\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m name, v\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-medium-finetuned\",\n",
    "\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-5, \n",
    "    num_train_epochs=10, # best number of epochs!\n",
    "    warmup_steps=30,\n",
    "\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "\n",
    "    predict_with_generate=True,           \n",
    "    generation_max_length=150,         \n",
    "\n",
    "    dataloader_num_workers=0,             \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f289df",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_meu = torchaudio.load(\"/workspace/whisper-math/english by me.m4a\")\n",
    "audio_arabic = torchaudio.load(\"/workspace/whisper-math/data/processed data/arabic/A/arabic 11.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29529fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)\n",
    "audio_meu = resampler(audio_meu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521395f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_arabic = resampler(audio_arabic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee271a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ar\", task=\"transcribe\")\n",
    "\n",
    "# load streaming dataset and read first audio sample\n",
    "input_features = processor(audio_arabic[0], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2c5d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af6f911aab04231baa312cc953c6f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c3d8d1ca8544fb93b2f5effe12d736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5467f7e2443a481098570936120ad29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-medium-finetuned/model.safetensors:   0%|          | 99.6kB / 3.06GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d426ac1c8e447594069a3d0a7d1e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-medium-finetuned/training_args.bin:  12%|#2        |   722B / 5.91kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/manushya-ai/whisper-medium-finetuned/commit/cc031347d094f55501a6ff094ee522777291f6bf', commit_message='End of training', commit_description='', oid='cc031347d094f55501a6ff094ee522777291f6bf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/manushya-ai/whisper-medium-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='manushya-ai/whisper-medium-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00ecef39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222fbb859146419e9f87b73623fd945c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/manushya-ai/whisper-medium-finetuned/commit/fc7b0397be7e88803eed0565b4a39ee6f890926d', commit_message='Upload processor', commit_description='', oid='fc7b0397be7e88803eed0565b4a39ee6f890926d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/manushya-ai/whisper-medium-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='manushya-ai/whisper-medium-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.push_to_hub(repo_id=\"manushya-ai/whisper-medium-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
